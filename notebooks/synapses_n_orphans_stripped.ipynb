{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181d621-92f7-48c6-ba4c-681868a66008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from caveclient import CAVEclient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from statsmodels.stats import proportion\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43b475-93aa-4457-8b95-dc7012c34cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tables(pre_df):\n",
    "    # using this as the only place to pull pre-syn's from because it's well-proofread\n",
    "    presyn_df = ['allen_v1_column_types_slanted']\n",
    "    df = client.materialize.query_table(presyn_df[0],split_positions=True)\n",
    "    # post-synaptic partners of starter cell\n",
    "    pre_root_id = np.array(pre_df.pt_root_id)[0]\n",
    "    syn_unfiltered = client.materialize.query_table('synapses_pni_2',\n",
    "                                                filter_equal_dict={'pre_pt_root_id':pre_root_id})\n",
    "    # if updated, this will change\n",
    "    correct_soma_table = client.info.get_datastack_info()['soma_table']\n",
    "    # x, y, and z will have their own columns\n",
    "    nuclei_unmasked = client.materialize.query_table(correct_soma_table,split_positions=True)\n",
    "    # new df of just neurons (no glial cells)\n",
    "    nuclei = nuclei_unmasked.query('cell_type == \"neuron\"').reset_index(drop=True)\n",
    "    # new column saying how many neurons have the same root_id\n",
    "    nuclei['num_soma'] = nuclei.groupby('pt_root_id').transform('count')['valid']\n",
    "    # mask the df to throw out merged nuclei (same root_id being assigned to multiple neurons)\n",
    "    mask_nuclei = nuclei['num_soma'] < 2\n",
    "    nuclei_full = nuclei[mask_nuclei].reset_index(drop=True)\n",
    "    # grabbing the unique root_id's of single-body neurons\n",
    "    unique_nuc = np.unique(nuclei_full.pt_root_id)\n",
    "    # masking the cell type table for only single-body neurons\n",
    "    soma_full = client.materialize.query_table('allen_soma_coarse_cell_class_model_v1',\n",
    "                                               filter_in_dict = {'pt_root_id':unique_nuc},\n",
    "                                               split_positions=True)\n",
    "    # masking the synapse table for only single-body neurons. these contain a ton of duplicates\n",
    "    syn_nuc_dup = syn_unfiltered.query(\"post_pt_root_id in @unique_nuc\").reset_index(drop=True)\n",
    "    # new column in synapse table = number of synapses per single soma\n",
    "    syn_nuc_dup['num_syn'] = syn_nuc_dup.groupby('post_pt_root_id').transform('count')['valid']\n",
    "    syn_nuc_dup = syn_nuc_dup.sort_values(by=['post_pt_root_id']).reset_index(drop=True)\n",
    "    # renaming bc 'size' is a function and it messes with grouping\n",
    "    syn_nuc_dup.rename(columns={'size':'sizes'}, inplace=True)\n",
    "    # dropping duplicates\n",
    "    syn_nuc = syn_nuc_dup.drop_duplicates(subset='post_pt_root_id', keep='first').reset_index(drop=True)\n",
    "    # grabbing every synaptic size and position and stacking them into a tuple so that each unique nucleus has a list of syn sizes\n",
    "    syn_nuc['ctr_pt_position'] = syn_nuc_dup.assign(ctr_pt_position=tuple(syn_nuc_dup.ctr_pt_position)).groupby('post_pt_root_id').ctr_pt_position.apply(list).reset_index(drop=True)\n",
    "    syn_nuc['sizes'] = syn_nuc_dup.assign(sizes=tuple(syn_nuc_dup.sizes)).groupby('post_pt_root_id').sizes.apply(list).reset_index(drop=True)\n",
    "    syn_nuc['sum_size'] = syn_nuc.apply(lambda row: sum(row.sizes), axis=1)\n",
    "    syn_nuc['ave_size'] = syn_nuc.apply(lambda row: row.sum_size / len(row.sizes), axis=1)\n",
    "    # renaming post_pt_root_id in order to merge correctly\n",
    "    syn_nuc.rename(columns={'post_pt_root_id':'pt_root_id'}, inplace=True)\n",
    "    # merge!\n",
    "    main = pd.merge(soma_full,syn_nuc,on='pt_root_id',how='left')\n",
    "    # these columns are useless to me\n",
    "    main = main.drop(columns=['id_x', 'id_y', 'valid_x', 'valid_y', 'pt_supervoxel_id', 'pre_pt_supervoxel_id', \n",
    "                          'post_pt_supervoxel_id', 'pre_pt_position', 'post_pt_position'])\n",
    "    main = main.fillna(0)\n",
    "    # add new columns for cartesian & radial distance to root_id's \n",
    "    main['d'] = Cartdistance_cell2cell(pre_df,main)\n",
    "    main['r'] = Raddistance_cell2cell(pre_df,main)\n",
    "    main['d_syn2pre'] = Cartdistance_syn2cell(main,pre_df)\n",
    "    main['r_syn2pre'] = Raddistance_syn2cell(main,pre_df)\n",
    "    main['d_syn2post'] = Cartdistance_syn2cell(main,main)\n",
    "    main['r_syn2post'] = Raddistance_syn2cell(main,main)\n",
    "    # grabbing the unique root_id's of single-body neurons in the synapse table\n",
    "    unique_syn_nuc = np.unique(syn_nuc.pt_root_id)\n",
    "    # new tables sorted from main of synaptic targets or non-synaptic neighbors of pre_root_id\n",
    "    syn = main.query('pt_root_id in @unique_syn_nuc').reset_index(drop=True)\n",
    "    nonsyn = main.query('pt_root_id not in @unique_syn_nuc').reset_index(drop=True)\n",
    "    return main,syn,nonsyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48886bcc-968c-481a-9d01-2748a8eaacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_spitter(main,df):\n",
    "    types = np.unique(main.cell_type)\n",
    "    cellarray = []\n",
    "    for i in range(len(types)):\n",
    "        new = df.query(f\"cell_type in @types[{i}]\").reset_index(drop=True)\n",
    "        cellarray.append(new)\n",
    "    return cellarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92fd5a-e01a-46aa-b709-56c068ba56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cartdistance_cell2cell(pre,post):\n",
    "    # adjusts coordinates to be in units of microns\n",
    "    xy = (4./1000)\n",
    "    z = (40./1000)\n",
    "    x_pre,y_pre,z_pre = pre.pt_position_x.values*xy,pre.pt_position_y.values*xy,pre.pt_position_z.values*z\n",
    "    x_pos,y_pos,z_pos = post.pt_position_x.values*xy,post.pt_position_y.values*xy,post.pt_position_z.values*z\n",
    "    d = np.zeros(len(post))\n",
    "    for i in range(len(post)):\n",
    "        d[i] = np.around(np.sqrt((x_pre-x_pos[i])**2 + (y_pre-y_pos[i])**2 + (z_pre-z_pos[i])**2),3)\n",
    "    return d\n",
    "\n",
    "def Raddistance_cell2cell(pre,post):\n",
    "    # adjusts coordinates to be in units of microns\n",
    "    xy = (4./1000)\n",
    "    z = (40./1000)\n",
    "    x_pre,z_pre = pre.pt_position_x.values*xy,pre.pt_position_z.values*z\n",
    "    x_pos,z_pos = post.pt_position_x.values*xy,post.pt_position_z.values*z\n",
    "    d = np.zeros(len(post))\n",
    "    for i in range(len(post)):\n",
    "        d[i] = np.around(np.sqrt((x_pre-x_pos[i])**2 + (z_pre-z_pos[i])**2),3)\n",
    "    return d\n",
    "\n",
    "\n",
    "def Cartdistance_syn2cell(syn_df,cell):\n",
    "    # adjusts coordinates to be in units of microns\n",
    "    xy = (4./1000)\n",
    "    z = (40./1000)\n",
    "    x_cell,y_cell,z_cell = cell.pt_position_x.values*xy,cell.pt_position_y.values*xy,cell.pt_position_z.values*z\n",
    "    distance = []\n",
    "    for i in range(len(syn_df)):\n",
    "        d = []\n",
    "        if syn_df.ctr_pt_position[i] == 0:\n",
    "            d.append(0)\n",
    "        else:\n",
    "            for j in range(len(np.array(syn_df.ctr_pt_position[i]))):\n",
    "                x_syn = np.array(syn_df.ctr_pt_position[i])[j][0]*xy\n",
    "                y_syn = np.array(syn_df.ctr_pt_position[i])[j][1]*xy\n",
    "                z_syn = np.array(syn_df.ctr_pt_position[i])[j][2]*z\n",
    "                if len(cell) == 1:\n",
    "                    dsyn = np.sqrt((x_syn-x_cell[0])**2 + (y_syn-y_cell[0])**2 + (z_syn-z_cell[0])**2)\n",
    "                else:\n",
    "                    dsyn = np.sqrt((x_syn-x_cell[i])**2 + (y_syn-y_cell[i])**2 + (z_syn-z_cell[i])**2)\n",
    "                d.append(np.around(dsyn,3))\n",
    "        distance.append(d)\n",
    "    return distance\n",
    "\n",
    "def Raddistance_syn2cell(syn_df,cell):\n",
    "    # adjusts coordinates to be in units of microns\n",
    "    xy = (4./1000)\n",
    "    z = (40./1000)\n",
    "    x_cell,z_cell = cell.pt_position_x.values*xy,cell.pt_position_z.values*z\n",
    "    distance = []\n",
    "    for i in range(len(syn_df)):\n",
    "        d = []\n",
    "        if syn_df.ctr_pt_position[i] == 0:\n",
    "            d.append(0)\n",
    "        else:\n",
    "            for j in range(len(np.array(syn_df.ctr_pt_position[i]))):\n",
    "                x_syn = np.array(syn_df.ctr_pt_position[i])[j][0]*xy\n",
    "                z_syn = np.array(syn_df.ctr_pt_position[i])[j][2]*z\n",
    "                if len(cell) == 1:\n",
    "                    dsyn = np.sqrt((x_syn-x_cell[0])**2 + (z_syn-z_cell[0])**2)\n",
    "                else:\n",
    "                    dsyn = np.sqrt((x_syn-x_cell[i])**2 + (z_syn-z_cell[i])**2)\n",
    "                d.append(np.around(dsyn,3))\n",
    "        distance.append(d)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc3632-651e-4d3b-ab29-316f8fea84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tables(pre_df,main,syn,nonsyn,r_interval,upper_distance_limit):\n",
    "    main_types = type_spitter(main,main)\n",
    "    syn_types = type_spitter(main,syn)\n",
    "    nonsyn_types = type_spitter(main,nonsyn)\n",
    "    \n",
    "    bins = np.array(range(0,upper_distance_limit,r_interval))\n",
    "    f_type,s_type = [],[]\n",
    "    for j in range(len(main_types)):\n",
    "        f,s = binomial_CI(main_types[j],bins)\n",
    "        f_type.append(f)\n",
    "        s_type.append(s)\n",
    "    return main_types,syn_types,nonsyn_types,f_type,s_type\n",
    "\n",
    "def prep_tables_thresh(pre_df,main,syn,nonsyn,r_interval,upper_distance_limit,threshold):\n",
    "    main['thresh'] = main.apply(lambda row: np.min(row.d_syn2post) < threshold, axis=1)\n",
    "    syn['thresh'] = syn.apply(lambda row: np.min(row.d_syn2post) < threshold, axis=1)\n",
    "    mask_m = main['thresh'] == True\n",
    "    mask_s = syn['thresh'] == True\n",
    "    main_thresh = main[mask_m].reset_index(drop=True)\n",
    "    syn_thresh = syn[mask_s].reset_index(drop=True)\n",
    "    \n",
    "    main_types_thresh = type_spitter(main_thresh,main_thresh)\n",
    "    syn_types_thresh = type_spitter(main_thresh,syn_thresh)\n",
    "    \n",
    "    bins = np.array(range(0,upper_distance_limit,r_interval))\n",
    "    f_type_thresh,s_type_thresh = [],[]\n",
    "    for j in range(len(main_types_thresh)):\n",
    "        f,s = binomial_CI(main_types_thresh[j],bins)\n",
    "        f_type_thresh.append(f)\n",
    "        s_type_thresh.append(s)\n",
    "    return main_thresh,syn_thresh,main_types_thresh,syn_types_thresh,f_type_thresh,s_type_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878ea7d-8fe6-40f9-bc57-ad1b4d3059fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_CI(df, bins):\n",
    "    fail = []\n",
    "    success = []\n",
    "    # no cells are less than 0 microns away, and this makes sure my arrays are the same size\n",
    "    fail.append(0)\n",
    "    success.append(0)\n",
    "    for i in range(len(bins)-1):\n",
    "        # masking for specific distance bin\n",
    "        masked_df = df[(df['r'] < bins[i+1]) & (df['r'] > bins[i])].reset_index(drop=True)\n",
    "        # starting the counter\n",
    "        f,s = 0,0\n",
    "        for j in range(len(masked_df)):\n",
    "            if masked_df['pre_pt_root_id'][j] == 0:\n",
    "                f += 1\n",
    "            else:\n",
    "                s += 1\n",
    "        # if there are zero cells in masked_df, 0's are appended\n",
    "        fail.append(f)\n",
    "        success.append(s)\n",
    "    return np.array(fail),np.array(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bbd88-c18b-4db9-b2ce-bd1b442fba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threepanels_pertype(pre,syn_types,nonsyn_types,s_type,f_type,unique_types,r_interval,upper_distance_limit,filename):\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    fig, ax = plt.subplots(len(unique_types),3)\n",
    "    fig.set_size_inches(12,26)\n",
    "    \n",
    "    bins = np.array(range(0,upper_distance_limit,r_interval))\n",
    "    x = bins-(r_interval/2)\n",
    "    for i in range(len(unique_types)):\n",
    "        sns.scatterplot(x=nonsyn_types[i].pt_position_x*(4/1000), y=nonsyn_types[i].pt_position_z*(40/1000), \n",
    "                        ax=ax[i,0], color='grey', alpha=.4, s=10)\n",
    "        sns.scatterplot(x=syn_types[i].pt_position_x*(4/1000), y=syn_types[i].pt_position_z*(40/1000), \n",
    "                        ax=ax[i,0], color='b', alpha=.9, s=10).set_xlabel(r'$\\mu$m (x vs z)')\n",
    "        sns.scatterplot(x=pre.pt_position_x*(4/1000), y=pre.pt_position_z*(40/1000), marker='*',color='r',s=200,\n",
    "                        ax=ax[i,0]).set_ylabel(unique_types[i], fontsize=16)\n",
    "        xrange = [int(pre.pt_position_x*(4/1000))-250,int(pre.pt_position_x*(4/1000))+250]\n",
    "        yrange = [int(pre.pt_position_z*(40/1000))-250,int(pre.pt_position_z*(40/1000))+250]\n",
    "        ax[i,0].set_xlim(xrange[0],xrange[1])\n",
    "        ax[i,0].set_ylim(yrange[0],yrange[1])\n",
    "        ax[i,0].set_aspect('equal')\n",
    "\n",
    "        method = 'wilson'\n",
    "        errorbars = sm.stats.proportion.proportion_confint(s_type[i],nobs=(s_type[i]+f_type[i]),method=method)\n",
    "        probability = (s_type[i]/(f_type[i]+s_type[i]))\n",
    "        ax[i,1].scatter(x=x, y=probability)\n",
    "        ax[i,1].errorbar(x=x, y=probability, yerr=(probability-errorbars[0],errorbars[1]-probability), fmt='-o') \n",
    "        ax[i,1].set_xlabel(r'$\\mu$m (radial)', fontsize=10)\n",
    "        ax[i,1].set_ylabel(\"Probability of Connection\", fontsize=10)\n",
    "        ax[i,1].set_ylim(-0.1,1.)\n",
    "        ax[i,1].grid()\n",
    "\n",
    "        ax[i,2].hist(x,bins=len(bins),weights=f_type[i]+s_type[i],density=False,label='Non-Synaptic', color='grey')\n",
    "        ax[i,2].hist(x,bins=len(bins),weights=s_type[i],density=False,label='Synaptic', color='blue')\n",
    "        ax[i,2].set_yscale('log')\n",
    "        ax[i,2].set_xlabel(r'$\\mu$m (radial)', fontsize=10)\n",
    "        ax[i,2].set_xlim(-10,(max(bins)+10))\n",
    "        ax[i,2].grid()\n",
    "        ax[i,2].set_ylabel(\"Log Frequency\", fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "    fig.savefig('./plots/{0:s}/{1:s}.pdf'.format(str(pre.cell_type.values[0]),filename))\n",
    "    \n",
    "def makeplots(pre,syn,syn_types,nonsyn_types,s_type,f_type,r_interval,upper_distance_limit,threshold):\n",
    "    if threshold == None:\n",
    "        for i in tqdm(range(len(pre))):\n",
    "            # this will give filename = 'BC-4587604586048560456748'\n",
    "            filename = '{0:s}-{1:s}'.format(str(np.array(pre[i].cell_type)[0]),str(np.array(pre[i].pt_root_id)[0]))\n",
    "            unique_types = np.unique(syn[i].cell_type)\n",
    "            threepanels_pertype(pre[i],syn_types[i],nonsyn_types[i],s_type[i],f_type[i],\n",
    "                                unique_types,r_interval,upper_distance_limit,filename)\n",
    "    else:\n",
    "        for i in tqdm(range(len(pre))):\n",
    "            # this will give filename = 'BC-4587604586048560456748'\n",
    "            filename = '{0:s}-{1:s}-{2:s}'.format(str(np.array(pre[i].cell_type)[0]),str(threshold),str(np.array(pre[i].pt_root_id)[0]))\n",
    "            unique_types = np.unique(syn[i].cell_type)\n",
    "            threepanels_pertype(pre[i],syn_types[i],nonsyn_types[i],s_type[i],f_type[i],\n",
    "                                unique_types,r_interval,upper_distance_limit,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef057e6-9727-40c5-aa01-d2523e5e179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CAVEclient(global_only=True)\n",
    "client = CAVEclient('minnie65_phase3_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afb85c-80e2-4878-a641-2d05f10388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "presyn_df = ['allen_v1_column_types_slanted']\n",
    "df = client.materialize.query_table(presyn_df[0],split_positions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20df49-f20a-4157-9032-d33e45efc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_root_ids = [864691135428608048]\n",
    "pre = []\n",
    "for i in range(len(pre_root_ids)):\n",
    "    pre_grab = df.query(f\"pt_root_id == @pre_root_ids[{i}]\")\n",
    "    pre.append(pre_grab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a11593-13e4-4798-84ba-60b5b4bd946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df.cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ae14c-1f55-4c21-bcc1-d74715e2125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = []\n",
    "pre_df = df.query(\"cell_type == '5P_IT'\").reset_index(drop=True)\n",
    "for i in range(len(pre_df)):\n",
    "    p = pre_df.iloc[[i]]\n",
    "    pre.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38cc2d-ff2e-41f7-a71a-316d699ef150",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dbe83-66f0-4ba0-842b-0ea193cc4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "main,syn,nonsyn = [],[],[]\n",
    "badroots = []\n",
    "for i in tqdm(range(len(pre))):\n",
    "    try:\n",
    "        m,s,n = build_tables(pre[i])\n",
    "        main.append(m)\n",
    "        syn.append(s)\n",
    "        nonsyn.append(n)\n",
    "    except:\n",
    "        badroots.append(pre[i])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c7a08-48f7-43b1-b0d6-ad0d9874f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 25\n",
    "up = 400\n",
    "threshold = 40\n",
    "main_types,syn_types,nonsyn_types,f_type,s_type = [],[],[],[],[]\n",
    "main_thresh,syn_thresh,main_types_thresh,syn_types_thresh,f_type_thresh,s_type_thresh = [],[],[],[],[],[]\n",
    "for i in tqdm(range(len(pre))):\n",
    "    beh = prep_tables(pre[i],main[i],syn[i],nonsyn[i],r,up)\n",
    "    main_types.append(beh[0])\n",
    "    syn_types.append(beh[1])\n",
    "    nonsyn_types.append(beh[2])\n",
    "    f_type.append(beh[3])\n",
    "    s_type.append(beh[4])\n",
    "    \n",
    "    bep = prep_tables_thresh(pre[i],main[i],syn[i],nonsyn[i],r,up,threshold)\n",
    "    main_thresh.append(bep[0])\n",
    "    syn_thresh.append(bep[1])\n",
    "    main_types_thresh.append(bep[2])\n",
    "    syn_types_thresh.append(bep[3])\n",
    "    f_type_thresh.append(bep[4])\n",
    "    s_type_thresh.append(bep[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b8257-ac05-4ec1-8788-b3855ccfa7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "makeplots(pre,main,syn_types,nonsyn_types,s_type,f_type,r,up,None)\n",
    "makeplots(pre,main,syn_types_thresh,nonsyn_types,s_type_thresh,f_type_thresh,r,up,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c1173-20d9-4cdb-aa3f-8cb3ba98e6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8dbb-d731-40ac-9718-05768bea99c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bc242-e614-4bbc-8aa7-e847e9e40847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f629099-ddec-4e84-88c5-b2dcc14f3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syn[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fb047-454f-4105-96cd-4df6715f59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findorphan_rootids(pre_df):\n",
    "    syn_unfiltered = client.materialize.query_table('synapses_pni_2',\n",
    "                                                filter_equal_dict={'pre_pt_root_id':pre_df.pt_root_id.values[0]})\n",
    "    correct_soma_table = client.info.get_datastack_info()['soma_table']\n",
    "    nuclei_unmasked = client.materialize.query_table(correct_soma_table,split_positions=True)\n",
    "    # new df of just neurons (no glial cells)\n",
    "#     nuclei = nuclei_unmasked.query('cell_type == \"neuron\"').reset_index(drop=True)\n",
    "#     # new column saying how many neurons have the same root_id\n",
    "#     nuclei['num_soma'] = nuclei.groupby('pt_root_id').transform('count')['valid']\n",
    "#     # mask the df to throw out merged nuclei (same root_id being assigned to multiple neurons)\n",
    "#     mask_nuclei = nuclei['num_soma'] < 2\n",
    "#     nuclei_full = nuclei[mask_nuclei].reset_index(drop=True)\n",
    "    unique_nuc = np.unique(nuclei_unmasked.pt_root_id)\n",
    "    syn_orphans = syn_unfiltered.query(\"post_pt_root_id not in @unique_nuc\").reset_index(drop=True)\n",
    "    unique_orphans = np.unique(syn_orphans.post_pt_root_id)\n",
    "    uniq_orph = np.array_split(unique_orphans,len(unique_orphans))\n",
    "    return syn_orphans,uniq_orph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a293-4db8-450d-8b8c-38494c234d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "orph,orph_rootids = findorphan_rootids(pre[5])\n",
    "print(\"\\nThere are {0:d} post-synaptic connections to {1:d} unique orphaned root_ids.\".format(len(orph),len(orph_rootids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e67e03-722f-4ec0-8e2f-58287547dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "orph = orph.drop(columns=['id', 'valid', 'pre_pt_supervoxel_id', \n",
    "                          'post_pt_supervoxel_id', 'pre_pt_position', 'post_pt_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98608655-3fee-4730-b10e-1c4208361516",
   "metadata": {},
   "outputs": [],
   "source": [
    "orph = orph.sort_values(by=['post_pt_root_id']).reset_index(drop=True)\n",
    "orph['num_syn'] = orph.groupby('post_pt_root_id')['post_pt_root_id'].transform('count')\n",
    "orph.rename(columns={'size':'sizes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2f5a1-954f-4d80-9428-6918c20ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn = orph.drop_duplicates(subset='post_pt_root_id', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b6a26-6a6e-4ed3-902e-52849bc4d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn['ctr_pt_position'] = orph.assign(ctr_pt_position=tuple(orph.ctr_pt_position)).groupby('post_pt_root_id').ctr_pt_position.apply(list).reset_index(drop=True)\n",
    "osyn['sizes'] = orph.assign(sizes=tuple(orph.sizes)).groupby('post_pt_root_id').sizes.apply(list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b4f90-f3c0-4aff-8a6a-c1f834f57b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn['sum_size'] = osyn.apply(lambda row: sum(row.sizes), axis=1)\n",
    "osyn['ave_size'] = osyn.apply(lambda row: row.sum_size / len(row.sizes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709f696-8fd9-4472-bbe3-b8dd56514436",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn['d'] = Cartdistance_syn2cell(osyn,pre[0])\n",
    "osyn['r'] = Raddistance_syn2cell(osyn,pre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20414f0-f303-4335-9ee2-048aaf524575",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn['d_ave'] = osyn.apply(lambda row: sum(row.d)/len(row.d), axis=1)\n",
    "osyn['d_range'] = osyn.apply(lambda row: np.max(row.d) - np.min(row.d), axis=1)\n",
    "osyn['r_ave'] = osyn.apply(lambda row: sum(row.r)/len(row.r), axis=1)\n",
    "osyn['r_range'] = osyn.apply(lambda row: np.max(row.r) - np.min(row.r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c714e97-4852-46ee-9f35-09c2b0b46c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "osyn[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeccbb-a5df-484e-9a71-1cd9db0a0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,6)})\n",
    "#weight = osyn.eval(\"num_syn / 40\").rename(\"weight\")\n",
    "g = sns.scatterplot(data=osyn,x='r_ave',y='num_syn',size='ave_size', sizes=(20, 200), legend= False);\n",
    "sns.scatterplot(data=osyn,x='r_ave',y='num_syn',size='sum_size', sizes=(20, 200), legend= False, alpha=.4)\n",
    "#sns.scatterplot(data=osyn,x='d_ave',y='num_syn',size='ave_size', sizes=(20, 200), legend= False)\n",
    "#g.set(xlim=(0, 250));\n",
    "#g.set(ylim=(0, 20));\n",
    "g.set_title('Average Radial Distance vs number of synapses per orphan (size=ave if blue, sum if yellow)', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a8643-d3f4-4d7e-a954-db3824c4cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,6)})\n",
    "#weight = osyn.eval(\"num_syn / 40\").rename(\"weight\")\n",
    "g = sns.scatterplot(data=osyn,x='r_range',y='num_syn',size='ave_size', sizes=(20, 200), legend= False);\n",
    "sns.scatterplot(data=osyn,x='r_range',y='num_syn',size='sum_size', sizes=(20, 200), legend= False, alpha=.4)\n",
    "#sns.scatterplot(data=osyn,x='d_ave',y='num_syn',size='ave_size', sizes=(20, 200), legend= False)\n",
    "#g.set(xlim=(0, 250));\n",
    "#g.set(ylim=(0, 20));\n",
    "g.set_title('Range of Radial Distance vs number of synapses per orphan (size=ave if blue, sum if yellow)', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82150429-8bf1-4b14-9629-f842de2f3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "ox,oy,oz = [],[],[]\n",
    "for i in range(len(orph)):\n",
    "    x = np.array(orph.ctr_pt_position[i])[0]*(4/1000)\n",
    "    y = np.array(orph.ctr_pt_position[i])[1]*(4/1000)\n",
    "    z = np.array(orph.ctr_pt_position[i])[2]*(40/1000)\n",
    "    ox.append(x)\n",
    "    oy.append(y)\n",
    "    oz.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb362604-4aab-4ba1-952c-7ab081177ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeabornFig2Grid():\n",
    "\n",
    "    def __init__(self, seaborngrid, fig,  subplot_spec):\n",
    "        self.fig = fig\n",
    "        self.sg = seaborngrid\n",
    "        self.subplot = subplot_spec\n",
    "        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n",
    "            isinstance(self.sg, sns.axisgrid.PairGrid):\n",
    "            self._movegrid()\n",
    "        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n",
    "            self._movejointgrid()\n",
    "        self._finalize()\n",
    "\n",
    "    def _movegrid(self):\n",
    "        \"\"\" Move PairGrid or Facetgrid \"\"\"\n",
    "        self._resize()\n",
    "        n = self.sg.axes.shape[0]\n",
    "        m = self.sg.axes.shape[1]\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n",
    "\n",
    "    def _movejointgrid(self):\n",
    "        \"\"\" Move Jointgrid \"\"\"\n",
    "        h= self.sg.ax_joint.get_position().height\n",
    "        h2= self.sg.ax_marg_x.get_position().height\n",
    "        r = int(np.round(h/h2))\n",
    "        self._resize()\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n",
    "\n",
    "        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n",
    "\n",
    "    def _moveaxes(self, ax, gs):\n",
    "        #https://stackoverflow.com/a/46906599/4124317\n",
    "        ax.remove()\n",
    "        ax.figure=self.fig\n",
    "        self.fig.axes.append(ax)\n",
    "        self.fig.add_axes(ax)\n",
    "        ax._subplotspec = gs\n",
    "        ax.set_position(gs.get_position(self.fig))\n",
    "        ax.set_subplotspec(gs)\n",
    "\n",
    "    def _finalize(self):\n",
    "        plt.close(self.sg.fig)\n",
    "        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def _resize(self, evt=None):\n",
    "        self.sg.fig.set_size_inches(self.fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce55b45-87f1-4dee-9400-be67fe3f1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.gridspec as gridspec\n",
    "# gs = gridspec.GridSpec(1, 2)\n",
    "# fig = plt.figure(figsize=(18,8))\n",
    "\n",
    "# sns.set_theme(style=\"ticks\")\n",
    "# g0 = sns.jointplot(x=ox, y=oz, kind='hex',color=\"#4CB391\")\n",
    "# #g0.set_title(\"Position of Orphan Synapses\", fontsize=16);\n",
    "# sns.scatterplot(x=pre[0].pt_position_x*(4/1000),y=pre[0].pt_position_z*(40/1000),marker='*',color='r',s=300).set_xlim(490,1130)\n",
    "\n",
    "# g1 = sns.jointplot(x=syn[0].pt_position_x*(4/1000), y=syn[0].pt_position_z*(40/1000),kind='hex',color=\"#4CB391\")\n",
    "# sns.scatterplot(x=pre[0].pt_position_x*(4/1000),y=pre[0].pt_position_z*(40/1000),marker='*',color='r',s=300).set_ylim(700,1130)\n",
    "\n",
    "# mg0 = SeabornFig2Grid(g0, fig, gs[0])\n",
    "# mg1 = SeabornFig2Grid(g1, fig, gs[1])\n",
    "\n",
    "# gs.tight_layout(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116125e4-9896-4509-8519-26790dd3143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_size_inches(16,10)\n",
    "\n",
    "ax[0].hist(ox, 30, label='Orphan Targets', density=True, alpha=0.6)\n",
    "ax[0].hist(syn[0]['pt_position_x']*(4/1000), 30, label='Somatic Targets', density=True, alpha=0.6)\n",
    "ax[0].axvline(np.array(pre[0]['pt_position_x']*(4/1000))[0], label='Pre-Syn Cell', c='r')\n",
    "ax[0].set_title(\"X Position\", fontsize=20)\n",
    "ax[0].set_xlabel(r'$\\mu$m', fontsize=14)\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].hist(oy, 30, label='Orphan Targets', density=True, alpha=0.6)\n",
    "ax[1].hist(syn[0]['pt_position_y']*(4/1000), 30, label='Somatic Targets', density=True, alpha=0.6)\n",
    "ax[1].axvline(np.array(pre[0]['pt_position_y']*(4/1000))[0], label='Pre-Syn Cell', c='r')\n",
    "ax[1].set_title(\"Y Position\", fontsize=20)\n",
    "ax[1].set_xlabel(r'$\\mu$m', fontsize=14)\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].hist(oz, 30, label='Orphan Targets', density=True, alpha=0.6)\n",
    "ax[2].hist(syn[0]['pt_position_z']*(40/1000), 30, label='Somatic Targets', density=True, alpha=0.6)\n",
    "ax[2].axvline(np.array(pre[0]['pt_position_z']*(40/1000))[0], label='Pre-Syn Cell', c='r')\n",
    "ax[2].set_title(\"Z Position\", fontsize=20)\n",
    "ax[2].set_xlabel(r'$\\mu$m', fontsize=14)\n",
    "ax[2].legend()\n",
    "ax[2].grid()\n",
    "\n",
    "ax[0].set_xlim(350,1100)\n",
    "ax[1].set_xlim(350,1100)\n",
    "ax[2].set_xlim(700,1150)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6939431-9596-4e12-b4ef-1da7c566b98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6b180-9249-4b8e-bacb-76e853e52860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5bd7a-e78d-4679-81ff-1b98f588be4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018690f-10d8-48a1-b33c-c20a122ed8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# o = []\n",
    "# for i in tqdm(range(len(orph_rootids))):\n",
    "#     orphans = client.materialize.query_table('synapses_pni_2',\n",
    "#                                              select_columns=['pre_pt_root_id','post_pt_root_id','size','ctr_pt_position'],\n",
    "#                                                filter_equal_dict={'post_pt_root_id':orph_rootids[i][0]})\n",
    "#     o.append(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faded0-5154-4326-b96a-584d2b3cc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#o_df = pd.concat(o,ignore_index=True)\n",
    "#o_df = o_df.drop(columns=['id', 'valid', 'pre_pt_supervoxel_id', 'post_pt_supervoxel_id', 'pre_pt_position', 'post_pt_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7d6a1-94a9-4283-a099-472be41617e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#o_df.to_pickle(\"./BC-orphans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e577dd-8833-4deb-9078-a2c01e1dfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#o_dup = pd.read_pickle('./BC-orphans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390c76b-a022-4e30-bf8f-7997fb3a98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_dup = o_dup.drop(columns=['id', 'valid', 'pre_pt_supervoxel_id', \n",
    "                          'post_pt_supervoxel_id', 'pre_pt_position', 'post_pt_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f98c95-f053-4ff5-b26b-ec71d2dbb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_dup = o_dup.sort_values(by=['post_pt_root_id']).reset_index(drop=True)\n",
    "o_dup['num_syn'] = o_dup.groupby('post_pt_root_id')['post_pt_root_id'].transform('count')\n",
    "o_dup.rename(columns={'size':'sizes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90ca9f-d841-4c83-87ff-c8705d7fbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = o_dup.drop_duplicates(subset='post_pt_root_id', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba614b8-0b5b-4a90-9ddd-3b0152e45816",
   "metadata": {},
   "outputs": [],
   "source": [
    "o['pre_pt_root_id'] = o_dup.assign(pre_pt_root_id=tuple(o_dup.pre_pt_root_id)).groupby('post_pt_root_id').pre_pt_root_id.apply(list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2919d-dfeb-4f82-a9d2-50a1b0e4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o['ctr_pt_position'] = o_dup.assign(ctr_pt_position=tuple(o_dup.ctr_pt_position)).groupby('post_pt_root_id').ctr_pt_position.apply(list).reset_index(drop=True)\n",
    "o['sizes'] = o_dup.assign(sizes=tuple(o_dup.sizes)).groupby('post_pt_root_id').sizes.apply(list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df032a5f-563e-4d6a-8a3f-59826c73457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#o_dup#.sort_values(by=['pre_pt_root_id']).reset_index(drop=True)\n",
    "blbl = np.unique(o_dup.pre_pt_root_id)\n",
    "print(\"There are {0:d} other pre-synaptic connections to the orphaned root id's.\".format(len(blbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e6d0e-e452-4025-ad5d-b6b0c7c91a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "o[0:3]#.sort_values(by=['pre_pt_root_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f13d7d-c399-40a2-a5dc-2199f6dd83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "o['d_syn2pre'] = Cartdistance_syn2cell(o,pre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a87f5-1b39-4130-a03b-74265d8dac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o['r_syn2pre'] = Raddistance_syn2cell(o,pre[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639df6e7-f3f2-4664-b32a-26c6347b8cfc",
   "metadata": {},
   "source": [
    "#### Should I find the distance to the other pre-synaptic connections? (would involve querying the soma table for 2.3 million neurons. I suppose some might not even be neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4c96b-a475-42e8-b7a9-0123e546604c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24425d36-2560-4090-87cd-9d3ce56c2fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e286606-42b0-4201-ae2a-bda0a04e019b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ee243-ab4a-4c48-ad93-d0bad10e9e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468c647-7bf0-4fee-b240-dbcfb6fe141b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d863c357-56de-4519-abe3-c4852883666e",
   "metadata": {},
   "source": [
    "- ~method~\n",
    "- ~depth slices~\n",
    "- ~depth vs radial distance~\n",
    "- ~add synapse locations to root_id in table~\n",
    "- ~add average synapse size to root_id in table~\n",
    "- ~array of distances for synapse to target (and synapse to pre_syn)~\n",
    "- quantify orphan targets, add uncertainty OR assign to excitatory\n",
    "- ~make the same plots but only for synapses within 40 Âµm of their target cell body position~\n",
    "- make the same plots but scale color w snapse count\n",
    "- add ave synapse size to depth vs radial distance plots\n",
    "- distribution of total number of synapses from the starter cell onto those cells per cell type\n",
    "- calculate angle to target cells from pre_synaptic cell / determine axon angle\n",
    "- measure drop-off of axon/dendrite overlap\n",
    "\n",
    "- normalize hist?\n",
    "- target cell's synapse position info? spread? volume?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbae51-d8c5-4f36-b12f-b04d274ed480",
   "metadata": {},
   "source": [
    "### From Casey:\n",
    "\n",
    "Orphans will happen, in principle, when the root id does not show up in the nucleus_neuron_svm table. \n",
    "\n",
    "There are two basic possibilities for an orphan:\n",
    "1. The orphan should connect to a cell in the dataset but there is an error in the segmentation that needs proofreading\n",
    "2. The orphan is a true orphan, part of a cell outside the dataset volume.\n",
    "\n",
    "For our measurements, Category 1 breaks down into more cateogries:\n",
    "\n",
    "    1a) Other synapses from the orphan's true host are accounted for, so the orphan will only add to the synapse count\n",
    "    \n",
    "    1b) The segmented cell does not receive other synapses from the seed interneuron, and thus there is a false lack of connection in the data until the orphan is attached.\n",
    "\n",
    "These all will have spatial biases (more orphans close to the dataset edge) and morphology biases (neurons that target cell bodies might have fewer orphans of type 1b, because dendrite fragments tend to start farther out on the cell)\n",
    "\n",
    "I suspect we will have to actually do some manual counting of these, in order to quantify our uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b5c95-2c54-4114-ac9f-c80c1bd5a476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
